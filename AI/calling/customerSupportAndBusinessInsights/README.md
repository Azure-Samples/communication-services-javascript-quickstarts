# ACS Client Calling SDK integration with Azure OpenAI
Customer interactions are the key to building trust, loyalty, and satisfaction. However, traditional communication process such as phone and video calls can be limited in their effectiveness and efficiency. Customers may have to wait for long periods, repeat their queries, or deal with unresponsive agents who aren't able to respond with the best answers. Businesses miss valuable insights, feedback, or opportunities to upsell or cross-sell their products or services.

Contact and support centers are often the first point of contact for customers who need assistance, information, or feedback from an organization. However, managing a large volume of inbound and outbound calls, emails, and chats can be challenging, costly, and stressful for both agents and customers. That's why many contact centers are looking for ways to leverage artificial intelligence (AI) to enhance their workflows, automate repetitive tasks, and provide faster and better service. In this article and the sample below, we will explore how Azure Communication Services can help contact centers use AI to improve support agent productivity, customer satisfaction, and reduce costs. 

Azure Communication Services offers multichannel communication APIs for adding voice, video, chat, text messaging/SMS, email, and more to developer’s applications and is the same underlying platform and infrastructure that powers Microsoft Teams and more the 300 million active users per day. These capabilities can be integrated into custom application experiences that use your brand’s identity and distinctiveness.   

This sample shows how to give call agents proactive on-screen talking points of how to best respond to their customers based on AI learning.

## Build a custom contact center solution to meet your needs

The integration of Azure Communication Services Client Calling SDK with Azure AI enables businesses to bring artificial intelligence into the client-calling flow. In this Contact Center as a Service model, the contact center support agent receives live recommendations from Azure Open AI on what to say next to best support the customer. Also, call sentiment and call summary that are generated by Azure Open AI can be used by businesses for visualizing call insight.

Imagine this scenario: you want to build or modernize your contact center, and your top priorities when doing so are customer satisfaction and agent efficiency. Your customer calls in and gets connected with a live support agent who leverages AI to obtain information which helps him with resolving the customer scenario. Additionally, to continuously improve agent efficiency, it’s important to gain insights on things like call sentiment and a call summary for later strategy and training purposes. The following sample shows how real-time intelligence can support a live agent with prompts of how best to handle customer issues and the business with insights:

1.	A Contoso customer calls the Contoso business support agent, who is using a Microsoft Team’s identity joining the call through the Contoso application built with Azure Communication Services Client calling SDK. We need Teams identity for closed captions functionality.
2. The Contoso support agent client gets the caption history from the chat between the customer and the agent from Azure Communication Services Client Calling SDK.
3.	The accumulated captions are sent from the client application to a back-end service built with Azure functions, that is set up to act as a gateway to Azure Open AI. Two prompts are set. The first one provides the Contoso agent suggestions along with customer data gathered from the conversation for customer support form filling while the second one gathers call sentiment and insights for business monitoring purposes.
4.	The agent suggestions received from the back end are shown on the Contoso support agent client application user interface. The suggestions returned will give the call agent the key talking points to communicate to the customer. The talking points will be optimized hints that are backed by AI. The customer information is gathered from the conversation by AI and is used to fill in support ticket creation forms to help the agent with support ticket creation. 
5.	Business insights such as call sentiment (positive/neutral/negative) and call insight summary are sent from the back-end service to Azure monitor, and log analytics for business visualization.

In short, Azure Communications Services client calling + Azure Open AI can revolutionize the way businesses interact with their customers. AI integration can generate insightful visualizations that businesses can leverage to make informed decisions. 

**Understanding the call flow**
![FHL - calling SDK + Open AI](https://github.com/Azure-Samples/communication-services-javascript-quickstarts/assets/101957302/18995d51-b410-444d-8915-db37c8e2d357)

## Easily creating a solution to help call agents
The entire sample project can be found at:
Front end: [communication-services-web-calling-tutorial at calling_AIBased_AgentSupport](link)
Back end: [openAIGateway](link)

Alternatively you can also set up this sample project from scratch following the instructions.

## Configuring ingesting of data to Azure Functions: 
1. You can configure the backend service using Azure function app following the instructions at [Getting started with Azure Functions | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-functions/functions-get-started?pivots=programming-language-csharp#create-your-first-function). This back end sample is built with [C#](https://learn.microsoft.com/en-us/dotnet/csharp/) programming language. You can use any of the [languages supported](https://learn.microsoft.com/en-us/azure/azure-functions/supported-languages?tabs=isolated-process%2Cv4&pivots=programming-language-csharp) by Azure functions to build this back end application.
2. You can create an Azure function `GetSuggestionForContosoSupportAgent`. This function accepts the call transcript between the customer and the support agent. The function connects to Azure OpenAI to fetch suggestions on what the agent should say next to better support the customer based on the conversation transcript so far.

You should be sure to add your Azure OpenAI credentials. Add `ContosoAgenSupportSystemPrompt` and `ContosoAgentSupportUserPrompt` to a constant file:

Refer to [Azure Open AI](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/working-with-models?tabs=powershell) documentaion to get started working with Azure open AI models.

```csharp
     [FunctionName("GetSuggestionForContosoSupportAgent")]
       public static async Task<IActionResult> HandleGetSuggestionForContosoSupportAgent(
       [HttpTrigger(AuthorizationLevel.Anonymous, "get", "post", Route = null)] HttpRequest req, ILogger log)
       {
           string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
           dynamic data = JsonConvert.DeserializeObject(requestBody);
           String transcript = data?.transcript;

           OpenAIClient client = new OpenAIClient(
                // Replace with your Azure Open AI resource URI and key credentials
                new Uri("<<https://your-azure-openai-resource.com/>>"),
                new AzureKeyCredential("<<your-azure-openai-resource-api-key>>"));


           var chatCompletionsOptions = new ChatCompletionsOptions()
           {
               Messages =
                  {
                       new ChatMessage(ChatRole.System, Constants.ContosoAgentSupportSystemPrompt),
                       new ChatMessage(ChatRole.User, transcript),
                       new ChatMessage(ChatRole.User, Constants.ContosoAgentSupportUserPrompt)
                   },
               Temperature = (float)1,
               MaxTokens = 800            
           };
           Response<ChatCompletions> response = null;
           try
           {
               // Replace with your Azure Open AI deployment name
               response = await client.GetChatCompletionsAsync(
               "<<your-deployment-name>>", chatCompletionsOptions);
           } catch(Exception e) 
           {
               Console.WriteLine(e);
           }

           return new OkObjectResult(response.Value.Choices[0].Message.Content);
       }

```

Prompt for GetSuggestionForContosoSupportAgent function:
```csharp
        // Prompt for ContosoAgentSupport
        public static string ContosoAgentSupportUserPrompt = @"From the above conversation between the support agent and the user,
                        Extract user content and fill in the requirements form data
                        If the user-provided content is incomplete, stuttered, or unclear, suggest the support agent with polite suggestions to clarify what was understood and what the agent should ask to fulfill the questions. 
                        The goal is to make sure the user details and issues are well understood and the required details are collected on the form.
                        If the date or mailing address is not valid, suggest agent to get the details from the user.
                        If the purchase date is older than 2 years from the current date, then mark product_under_warranty form data as false.
                        Suggest an agent with troubleshooting suggestions.
                        The response should be a JSON format.
                     
                                {
                                  ""requirements"": {
                                   ""name_provided"": true/fale,
                                    ""mailing_address_provided"": true/false,
                                    ""date_of_purchase_provided"": true/false,
                                    ""phone_number_provided"": true/false,
                                    ""issue_outlined"": true/false
                                  },
                                  ""form_data"": {
                                    ""name"": ""..."",
                                    ""address"": ""..."",
                                    ""phone_number"": ""..."",
                                    ""date_of_purchase"": ""dd/mm/yyyy"",
                                    ""issue_description"": ""..."",
                                   ""product_under_warranty"":"""",
                                   ""issue_resolved_oncall"": """",
                                   ""support_ticket_number"": """"
                                  },
                                  ""suggested_reply"": ""..."",
                                } ";
        public static string ContosoAgentSupportSystemPrompt = "You are an AI assistant assisting a support agent, listening to the conversation between the support agent and the user.";
```

3. Create a function called `CallInSights`. This function accepts the callId and entire call transcript between the customer and the support agent. The function fetches call sentiment (positive, negative, neutral) and call summary from Azure OpenAI based on the call transcript. Update the Azure Open AI Client creation step with your Azure OpenAI credentials. Add the `sentimentScoreSystemPrompt` and `sentimentScoreUserPrompt` to a constant file.

```csharp
[FunctionName("CallInSights")]
        public async Task<IActionResult> CallInSights(
        [HttpTrigger(AuthorizationLevel.Anonymous, "get", "post", Route = null)] HttpRequest req, ILogger log)
        {
            log.LogInformation("Processing call insights on transcription request");


            string requestBody = await new StreamReader(req.Body).ReadToEndAsync();
            dynamic data = JsonConvert.DeserializeObject(requestBody);
            String transcript = data?.transcript;

            // Replace with your Azure Open AI resource URI and key credentials
            OpenAIClient openAIClient = new OpenAIClient(
                new Uri("<<https://your-azure-openai-resource.com/>>"),
                new AzureKeyCredential("<<your-azure-openai-resource-api-key>>"));

            var chatCompletionsOptions = new ChatCompletionsOptions()
            {
                Messages = 
                   {
                        new ChatMessage(ChatRole.System, Constants.sentimentScoreSystemPrompt),
                        new ChatMessage(ChatRole.User, transcript),
                        new ChatMessage(ChatRole.User, Constants.sentimentScoreUserPrompt),
                    },
                Temperature = (float)1,
                MaxTokens = 800
            };

            // Replace with your Azure Open AI deployment name
            Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(
            "<<your-deployment-name>>", chatCompletionsOptions);

            return new OkObjectResult(response.Value.Choices[0].Message.Content);
        }
```


```csharp
// Prompts for call insight generation 
public static string sentimentScoreSystemPrompt = "You are an AI assistant listening to the conversation between the support agent and the user.";
public static string sentimentScoreUserPrompt = @"From the above conversation between the agent and the user,
                Generate a sentiment score Positive, Negative or Neutral, based on the conversation, customer satisfaction, and agent ability to support the user.
                Geneate a call insight. 

                The response should be a JSON format.
                    {
                        ""callSentiment"": """",
                        ""callInsight"": """"
                    }";
```
4.  Publish the openaigateway function app using these [instructions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-develop-vs?tabs=isolated-process).


## Easily build a user interface to show AI insights

1. You can create agent the user interface that is built on top of the Azure Communication Services web client calling [sample](https://docs.microsoft.com/en-us/azure/communication-services/samples/web-calling-sample) tutorial. Follow the instructions on the Calling Sample app at [GitHub](https://github.com/Azure-Samples/communication-services-web-calling-tutorial) and set up the web application.

2. Update the web application to include the following changes.
Update `CallCaptions.js`: Add three state variables that represent: Sending data to backend AI is turned on or off, represent if the agent is speaking, and to represent if the customer is speaking and if AI functionality is enabled.
    ```js
        const [communicationAI, setCommunicationAI] = useState(false);
        const [isAgentSpeaking, setIsAgentSpeaking] = useState(false);
        const [isUserSpeaking, setIsUserSpeaking] = useState(false);
    ```
    - Add a toggle button component to turn on AI functionality. When AI functionality is turned on, show the new Communication AI component that is populated with the response from Azure Open AI.
    ```js
        return (
            <>
                {captions && <SpokenLanguageDropdown />}
                {captions && captions.captionsType === 'TeamsCaptions' && <CaptionLanguageDropdown />}
                <div className="scrollable-captions-container">
                    <div id="captionsArea" className="captions-area">
                    </div>
                </div>
                <div className="participants-panel mt-1 mb-3">
                    <Toggle label= {
                        <div>
                            Communication AI {' '}
                            <TooltipHost content= {`Turn on Communication AI`}>
                                <Icon iconName="Info" aria-label="Info tooltip" />
                            </TooltipHost>
                        </div>
                    }
                        styles= {{
                            text: {color: '#edebe9'},
                            label: {color: '#edebe9'},
                        }}
                        inlineLabel
                        onText="On"
                        offText="Off"
                        defaultChecked={communicationAI}
                        onChange= {() => { setCommunicationAI(oldValue => !oldValue)}}
                    />
    
                    {
                        communicationAI &&
                        <CommunicationAI call={call} isAgentSpeaking={isAgentSpeaking} isUserSpeaking={isUserSpeaking} />
                    }
                </div>
            </>
        );
    ```
     
- Create file CommunicationAI.js to handle AI related functionality under src/MakeCall/CommunicationAI.
When the user stops speaking, send the captions gathered so far to Azure Open AI. The response received from AI suggests key talking points for the Agent on what to say next to best support the customer and customer data gathered from the conversation to fill in the support form.

    ```js
    import React, { useState, useEffect } from "react";
    import { Dropdown } from '@fluentui/react/lib/Dropdown';
    import { utils, acsOpenAiPromptsApi } from "./Utils";
    import {AgentSupportForm} from "./AgentSupportForm"

    const CommunicationAI = ({ call, isAgentSpeaking, isUserSpeaking }) => {
        const [showSpinner, setShowSpinner] = useState(false);

        // Summary
        const [lastSummary, setLastSummary] = useState("");
        const [captionsSummaryIndex, setCaptionsSummaryIndex] = useState(0);

        // Feedback
        const [lastFeedBack, setLastFeedBack] = useState("");
        const [captionsFeedbackIndex, setCaptionsFeedbackIndex] = useState(0);

        // Sentiment
        const [lastSentiment, setLastSentiment] = useState("");
        const [captionsSentimentIndex, setCaptionsSentimentIndex] = useState(0);

        // Support Agent
        const [lastSupportAgentResponse, setLastSupportAgentResponse] = useState("");
        const [setCaptionsSupportAgentResponseIndex] = useState(0);

        const [promptMessage, setPromptMessage] = useState("");

        const [dropDownLabel, setDropDownLabel] = useState("")

        const [agentDebounceCounterRunning, setAgentDebounceCounterRunning] = useState(false);
        const [userDebounceCounterRunning, setUserDebounceCounterRunning] = useState(false);

        const [userName, setUserName] = useState("");
        const [address, setAddress] = useState("");
        const [phoneNumber, setPhoneNumber] = useState("");
        const [dateOfPurchase, setDateOfPurchase] = useState("");
        const [issue, setIssue] = useState("");
        const [productUnderWarranty, setProductUnderWarranty] = useState(false);
        const [issueTicket, setIssueTicket] = useState("");

        const options = [
            { key: 'getSummary', text: 'Get Summary' },
            { key: 'getPersonalFeedBack', text: 'Get Personal Feedback' },
            { key: 'getSentiments', text: 'Get Sentiment Feedback' },
            { key: 'getSuggestionForSupportAgent', text: 'Get Suggestion for Agent' },
        ]
        let agentDebounceTimeoutFn;
        let userDebounceTimeoutFn;
        let displayName = "Agent"

        useEffect(() => {
            call.on('stateChanged', () => {
                if (call.state === 'Disconnected') {
                    callInsight(call.id);
                }
            });
        }, []);

        useEffect(() => {
            if (dropDownLabel == "") {
                setShowSpinner(false); 
                return
            }
            clearTimeout(agentDebounceTimeoutFn);
            if (dropDownLabel != "getSuggestionForSupportAgent") {
                if (isAgentSpeaking && !agentDebounceCounterRunning) {
                    const message = "FeedBack will be retrieved after you finish talking";
                    !showSpinner && setShowSpinner(true);
                    setPromptMessage(message);
                } else {
                    if (agentDebounceCounterRunning) {
                        agentDebounceTimeoutFn = setTimeout(() => {
                            setAgentDebounceCounterRunning(false);
                        }, 5000);
                    } else {
                        dropDownHandler();
                    }
                }
            }
            return () => {
                clearTimeout(agentDebounceTimeoutFn);
            }
        }, [isAgentSpeaking, agentDebounceCounterRunning, dropDownLabel]);

        useEffect(() => {
            if (dropDownLabel == "") {
                setShowSpinner(false); 
                return
            }
            clearTimeout(userDebounceTimeoutFn);
            if (isUserSpeaking && dropDownLabel == "getSuggestionForSupportAgent" && !userDebounceCounterRunning) {
                const message = "Support Suggestion will be retrieved after User finishes talking";
                !showSpinner && setShowSpinner(true);
                setPromptMessage(message);
            } else {
                if (userDebounceCounterRunning) {
                    userDebounceTimeoutFn = setTimeout(() => {
                        setUserDebounceCounterRunning(false);
                    }, 5000);
                } else {
                    dropDownHandler();
                }
            }
            return () => {
                clearTimeout(userDebounceTimeoutFn);
            }
        }, [isUserSpeaking, userDebounceCounterRunning, dropDownLabel]);

        const dropDownHandler = async () => {
            dropDownLabel != "" && !showSpinner && setShowSpinner(true)
            setPromptMessage("Waiting for the AI response...");
            switch (dropDownLabel) {
                case "getSummary":
                    await getSummary().finally(() => setShowSpinner(false));
                    break;
                case "getPersonalFeedBack":
                    await getPersonalFeedback().finally(() => setShowSpinner(false));
                    break;
                case "getSentiments":
                    await getSentiment().finally(() => setShowSpinner(false));
                    break;
                case "getSuggestionForSupportAgent":
                    await getSuggestionForSupportAgent().finally(() => setShowSpinner(false));
                    break;
            }
        }

        // Get call summary based on call conversation
        const getSummary = async () => {
            try {
                const currentCaptionsData = window.captionHistory.slice(captionsSummaryIndex);
                let response = await utils.sendCaptionsDataToAcsOpenAI(acsOpenAiPromptsApi.summary, displayName, lastSummary, currentCaptionsData);
                let content = response.choices[0].message.content;
                console.log(`getSummary summary ===> ${JSON.stringify(response)}`)
                setLastSummary(content);
                setCaptionsSummaryIndex(window.captionHistory.length);
                displayResponse(content);
            } catch (error) {
                console.error(JSON.stringify(error))
            }
        }

        // Get Personal feedback based on call conversation
        const getPersonalFeedback = async () => {
            try {
                const currentCaptionsData = window.captionHistory.slice(captionsFeedbackIndex);
                let response = await utils.sendCaptionsDataToAcsOpenAI(acsOpenAiPromptsApi.feedback, displayName, lastFeedBack, currentCaptionsData)
                let content = response.choices[0].message.content;
                console.log(`getPersonalFeedback ===> ${JSON.stringify(response)}`)
                setLastFeedBack(content);
                setCaptionsFeedbackIndex(window.captionHistory.length);
                displayResponse(content);
            } catch(error) {
                console.error(JSON.stringify(error))
            }
        }

        // Get Call Sentiment based on call conversation
        const getSentiment = async () => {
            try {
                const currentCaptionsData = window.captionHistory.slice(captionsSentimentIndex);
                let response = await utils.sendCaptionsDataToAcsOpenAI(acsOpenAiPromptsApi.sentiment, displayName, lastSentiment, currentCaptionsData)
                let content = response.emotions && response.emotions.join(", ")
                let callToAction = response.call_to_action;
                if (!content || !content.length) {
                    content = "Neutral" //default is no senitment is detected
                }
                if (callToAction) {
                    content += "\nRecommended Action:\n"
                    content += callToAction;
                } 
                console.log(`getSentimentt ===> ${JSON.stringify(response)}`)
                setLastSentiment(content);
                setCaptionsSentimentIndex(window.captionHistory.length);
                displayResponse(content);
            } catch(error) {
                console.error(JSON.stringify(error))
            }
        }

        // Get suggestion for support agent based on customer converstion
        const getSuggestionForSupportAgent = async () => {
            try {
                let response = await utils.sendCaptionsDataToAcsOpenAI(acsOpenAiPromptsApi.supportAgent, 
                        displayName, lastSupportAgentResponse, window.captionHistory, true)
                let content = response.suggested_reply;
                console.log(`getSuggestionForSupportAgent ===> ${JSON.stringify(response)}`)
                console.log(`form_data ===> ${JSON.stringify(response.form_data)}`)
                retrieveFormData(response.form_data)
                setLastSupportAgentResponse(content);
                setCaptionsSupportAgentResponseIndex(window.captionHistory.length);
                displayResponse(content);
            } catch(error) {
                console.error(JSON.stringify(error))
            }
        }

        // Get Call Insights from the call conversation
        const callInsight = async (callId) => {
            await utils.sendCaptionsDataToAcsOpenAI(acsOpenAiPromptsApi.callInsights, displayName, '', window.captionHistory, true, callId);
        }

        const retrieveFormData = (form_data) => {
            if (form_data.name && form_data.name != 'N/A' && form_data.name != userName) {
                setUserName(form_data.name)
            }

            if (form_data.address && form_data.address != 'N/A' && form_data.address != address) {
                setAddress(form_data.address)
            }

            if (form_data.phone_number && form_data.phone_number != 'N/A' && form_data.phone_number != phoneNumber) {
                setPhoneNumber(form_data.phone_number)
            }

            if (form_data.date_of_purchase && form_data.date_of_purchase != 'N/A' && form_data.date_of_purchase != dateOfPurchase) {
                setDateOfPurchase(form_data.date_of_purchase)
            }

            if (form_data.issue_description && form_data.issue_description != 'N/A' && form_data.issue_description != issue) {
                setIssue(form_data.issue_description)
            }

            if (form_data.product_under_warranty && form_data.product_under_warranty != 'N/A' && form_data.product_under_warranty != productUnderWarranty) {
                setProductUnderWarranty(form_data.product_under_warranty)
            }

            if (form_data.support_ticket_number && form_data.support_ticket_number != 'N/A' && form_data.support_ticket_number != issueTicket) {
                setIssueTicket(form_data.support_ticket_number)
            }
        }

        const onChangeHandler = (e, item) => {
            setDropDownLabel(item.key);
        }

        const displayResponse = (responseText) => {
            let captionAreasContainer = document.getElementById(dropDownLabel);

            if(!responseText || !responseText.length) {return;}

            if (dropDownLabel == "getSuggestionForSupportAgent" || dropDownLabel == "getSentiments") {
                captionAreasContainer.style['font-size'] = '13px';
                captionAreasContainer.innerText  = responseText;
            } else {
                let aiResponseContent = document.createElement('div');
                aiResponseContent.style['borderBottom'] = '1px solid';
                aiResponseContent.style['padding'] = '10px';
                aiResponseContent.style['whiteSpace'] = 'pre-line';
                aiResponseContent.style['color'] = 'white';
                aiResponseContent.style['font-size'] = '12px';
                aiResponseContent.textContent = responseText;
                captionAreasContainer.appendChild(aiResponseContent);
            }
        }

        return (
            <>
                <div id="" className="">
                    {
                        showSpinner &&
                        <div>
                            <div className="loader inline-block"> </div>
                            <div className="ml-2 inline-block">
                                {
                                    promptMessage
                                }
                            </div>
                        </div>
                    }
                    <Dropdown
                        placeholder="Select an option"
                        label={dropDownLabel}
                        options={options}
                        styles={{ dropdown: { width: 300 }, }}
                        onChange={onChangeHandler}
                    />
                </div>

                <div id="communicationResponse">
                    {
                        dropDownLabel == "getSummary" && 
                        <div className="scrollable-captions-container">
                            <div id="getSummary" className="ai-captions-area">
                            </div>
                        </div>
                    }

                    {
                        dropDownLabel == "getSentimentt" && 
                        <div className="scrollable-captions-container">
                            <div id="getSentimentt" className="ai-captions-area">
                            </div>
                        </div>
                    }

                    {
                        dropDownLabel == "getPersonalFeedback" && 
                        <div className="scrollable-captions-container">
                            <div id="getPersonalFeedback" className="ai-captions-area">
                            </div>
                        </div>
                    }

                    {
                        dropDownLabel == "getSuggestionForSupportAgent" && 
                        <div className="card">
                            <div className="ms-Grid">
                                <div className="ms-Grid-row">
                                    <div className="scrollable-captions-container ms-Grid-col ms-Grid-col ms-sm6 ms-md6 ms-lg6">
                                        <div id="getSuggestionForSupportAgent" className="captions-area">
                                            {lastSupportAgentResponse}
                                        </div>
                                    </div>
                                    {lastSupportAgentResponse && <AgentSupportForm 
                                        name = {userName}
                                        address = {address}
                                        phoneNumber = {phoneNumber}
                                        dateOfPurchase = {dateOfPurchase}
                                        issue = {issue}
                                        productUnderWarranty = {productUnderWarranty} 
                                        issueTicket = {issueTicket}
                                    />}
                                </div>
                            </div>
                        </div>
                    }
                </div>
            </>
        );
    };

    export default CommunicationAI;
    ```

    4. Create a file `Utils.js` under `src/MakeCall/CommunicationAI` for the `sendCaptionsDataToAcsOpenAI` function that sends the captions data and received intelligence from Open AI. You need to replace the base URL to point back end function app URL.

    ```js
    import axios from 'axios';
    export const acsOpenAiPromptsApi = {
        base: 'https://fhlopenaicalling.azurewebsites.net/api/',
        summary: 'getSummary',
        feedback: 'getPersonalFeedback',
        sentiment: 'GetSentiments',
        supportAgent: 'getSuggestionForSupportAgent',
        callInsights: 'CallInsights',
        getBriefSummary: 'GetBriefSummary'
    }

    export const utils = {
        sendCaptionsDataToAcsOpenAI: async (apiEndpoint, participantName, lastResponse, newCaptionsData, isTranscriptType = false, callId ="") => {
            let response = await axios({
                url: acsOpenAiPromptsApi.base + apiEndpoint,
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': "*",
                    'X-Requested-With': 'XMLHttpRequest',
                },
                data: (isTranscriptType || apiEndpoint === acsOpenAiPromptsApi.callInsights) ?
                    {
                        "transcript": newCaptionsData.join(' '),
                        "callId": callId
                    } :
                    {
                        "CurrentParticipant": participantName,
                        "Captions": JSON.stringify(newCaptionsData),
                        "LastSummary": JSON.stringify(lastResponse),
                    }
            });
            if (response.status === 200) {
                return response.data;
            } else {
                console.log("Error message");
                console.log(response);
            }
        }
    };
    ```

    5. The CommunicationAI UI component would be filled with the response from AI. It would show a suggestion for the Agent to best support the customer and an AgentSupportForm component that is filled with the customer data that AI gathered from the conversation. The AgentSupportForm.js needs to be created as under src/MakeCall/CommunicationAI.

    ```js
    import React, { useEffect, useState } from "react";
    import {
        TextField, PrimaryButton, Checkbox, MessageBar,
        MessageBarType,
    } from 'office-ui-fabric-react'
    import { v4 as uuidv4 } from 'uuid';

    export const AgentSupportForm = ({name, address, phoneNumber, dateOfPurchase, issue, productUnderWarranty}) => {
        const [userFullName, setUserFullName] = useState("");
        const [userAddress, setUserAddress] = useState("");
        const [userPhoneNumber, setUserPhoneNumber] = useState("");
        const [userDateOfPurchase, setUserDateOfPurchase] = useState("");
        const [IssueDescription, setIssueDescription] = useState("");
        const [underWarranty, setUnderWarranty] = useState(false);
        const [issueTicket, setIssueTicket] = useState("");
        const [isSubmitted, setIsSubmitted] = useState(false)

        useEffect(()=> {
            if (name !=  userFullName) {setUserFullName(name)}
            if (address !=  userAddress) {setUserAddress(address)}
            if (phoneNumber !=  userPhoneNumber) {setUserPhoneNumber(phoneNumber)}
            if (dateOfPurchase !=  userDateOfPurchase) {setUserDateOfPurchase(dateOfPurchase)}
            if (issue !=  IssueDescription) {setIssueDescription(issue)}
            if (productUnderWarranty !=  underWarranty) {setUnderWarranty(productUnderWarranty)}

            if (userFullName && userAddress && userPhoneNumber && userDateOfPurchase && IssueDescription) {
                console.log(`CHUK_TICKET === Updating Ticket number`);
                setIssueTicket(uuidv4())
            } else {
                console.log(`CHUK_TICKET === NOT Updating Ticket number`);
            }
        }, [name, address, phoneNumber, dateOfPurchase, issue, productUnderWarranty])

        const handleSubmit = (e) => {
            e.preventDefault();
            setIsSubmitted(true)
            setUserFullName("");
            setUserAddress("")
            setUserPhoneNumber("")
            setUserDateOfPurchase("")
            setIssueDescription("")
            setUnderWarranty(false)
            setIssueTicket("")
            setTimeout(() => {
                setIsSubmitted(false)
            }, 5000)
        }

        const FormSubmitted = () => {
            return (<MessageBar
            messageBarType={MessageBarType.success}
            isMultiline={false}
            >
            Ticket created successfully
            </MessageBar>)
        };

        return <>
            <div className="ms-Grid-col ms-Grid-col ms-sm6 ms-md6 ms-lg6" >
                {isSubmitted && FormSubmitted()}
                <div className="ms-Grid-row">
                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="FullName"
                            value={userFullName}
                            className="text-left"
                            onChange={(e) => { setUserFullName(e.target.value)}} 
                        />
                    </div>
                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="Address"
                            value={userAddress}
                            className="text-left"
                            onChange={(e) => { setUserAddress(e.target.value)}} 
                        />
                    </div>
                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="PhoneNumber" 
                            value={userPhoneNumber}
                            className="text-left"
                            onChange={(e) => { setUserPhoneNumber(e.target.value)}}  />
                    </div>
                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="Date of Purchase" 
                            value={userDateOfPurchase}
                            className="text-left"
                            onChange={(e) => { setUserDateOfPurchase(e.target.value)}}  />
                    </div>
                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="Issue Description" 
                            multiline rows={5}
                            value={IssueDescription}
                            className="text-left"
                            onChange={(e) => { setIssueDescription(e.target.value)}}  />
                    </div>
                    <div className="ms-Grid-row">
                        <Checkbox label="Product under Warranty"  checked={underWarranty} onChange={(e, checked) => {setUnderWarranty(checked)}} />
                    </div>

                    <div className="ms-Grid-row">
                        <TextField
                            placeholder="Issue Ticket#"
                            value={issueTicket}
                            className="text-left"
                            onChange={(e) => { setIssueTicket(e.target.value)}}
                        />
                    </div>
                </div>
                <div className="ms-Grid-row">
                    <div className="ms-Grid-col">
                        <PrimaryButton className="primary-button mt-5 text-left"

                            onClick={(e) => {handleSubmit(e)}}>
                                Submit Ticket
                        </PrimaryButton>
                    </div>
                </div>
            </div>
        </>
    }
    ```

## Exposing AI generated feed back to the user interface
 A support agent must turn on closed captions. The agent must also enable content sending to the AI engine by selecting `Get Suggestion for Agent`. The application then starts sending closed captions gathered. The output is Artificial intelligence aided suggestions and AI extracted customer data received from the API end point.


![alt text](image.png)

The application also sends out accumulated closed captions of the entire conversation between the agent and the customer just before the call ends, to the backend service `CallInSights` API end point, where artificial intelligence aided call insights such as call sentiment (positive/negative/neutral) and call summary gets generated. The insight can be sent out to log analytics for logging and Azure monitor for Business visualization. For a deeper understanding and insights on how to generate visualization based on telemetry follow the Azure.Monitor.Ingestion Namespace at [Azure for .NET Developers | Microsoft Learn](https://learn.microsoft.com/en-us/dotnet/api/azure.monitor.ingestion?view=azure-dotnet) & [Azure Monitor overview - Azure Monitor | Microsoft Learn](https://learn.microsoft.com/en-us/azure/azure-monitor/overview)

Log analytics – call insights:
![alt text](image-1.png)

Azure monitor visualization for Business:
![alt text](image-2.png)
![alt text](image-3.png)

As a reminder this entire sample code can be found at:
- Front end: [FrontEndSample](https://github.com/Azure-Samples/communication-services-javascript-quickstarts/tree/main/AI/calling/customerSupportAndBusinessInsights/frontEnd/webCallingApp)
- Back end: [OpenAIGateway](https://github.com/Azure-Samples/communication-services-javascript-quickstarts/tree/main/AI/calling/customerSupportAndBusinessInsights/backEnd/openAIGateway)

## Summary
This sample shows how you can quickly and easily give support agents the tools and talking points that help answer their customer's questions. By using Azure Communications Services and empowering the Open AI capabilities the result creates a revolutionary solution that combines the power of cloud-based communication and artificial intelligence. Azure Communications Services client calling enables businesses to embed voice and video calling features into their applications, websites, or mobile devices, without the need for complex infrastructure or coding. Open AI provides a suite of AI tools that can analyze, understand, and generate natural language, speech, and images, enhancing the quality and richness of customer interactions.